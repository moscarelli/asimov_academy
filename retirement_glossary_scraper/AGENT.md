# Autonomous Agent Documentation

## Overview

This project now includes a **true autonomous agent** that can build the retirement knowledge base by making its own decisions, rather than following a fixed pipeline.

## Agent vs Pipeline Comparison

| Feature | Autonomous Agent | Pipeline (main.py) | Legacy Script |
|---------|-----------------|-------------------|---------------|
| **Decision Making** | âœ… Plans its own steps | âŒ Fixed sequence | âŒ Fixed sequence |
| **Adaptation** | âœ… Adjusts based on results | âŒ No adaptation | âŒ No adaptation |
| **Memory** | âœ… Remembers past work | âŒ No memory | âŒ No memory |
| **Goal Awareness** | âœ… Knows when done | âŒ Runs all steps | âŒ Runs all steps |
| **Quality Checks** | âœ… Validates each step | âŒ No validation | âŒ No validation |
| **Error Recovery** | âœ… Adapts to failures | âŒ Continues blindly | âŒ Continues blindly |
| **Efficiency** | âœ… Avoids duplicate work | âš ï¸ Can re-process | âš ï¸ Can re-process |

## How the Agent Works

### 1. Agent Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Autonomous Agent (LLM Brain)        â”‚
â”‚  - Reasons about current state          â”‚
â”‚  - Plans next actions                   â”‚
â”‚  - Selects appropriate tools            â”‚
â”‚  - Learns from results                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â”œâ”€â”€> TOOLS (Agent's Capabilities)
                  â”‚    â”œâ”€â”€ analyze_website()
                  â”‚    â”œâ”€â”€ scrape_urls()
                  â”‚    â”œâ”€â”€ check_content_quality()
                  â”‚    â”œâ”€â”€ process_content()
                  â”‚    â”œâ”€â”€ index_to_database()
                  â”‚    â”œâ”€â”€ verify_indexing()
                  â”‚    â”œâ”€â”€ search_knowledge_base()
                  â”‚    â”œâ”€â”€ assess_progress()
                  â”‚    â””â”€â”€ get_memory_summary()
                  â”‚
                  â””â”€â”€> MEMORY (Agent's Brain)
                       â”œâ”€â”€ Past sessions
                       â”œâ”€â”€ Scraped URLs
                       â”œâ”€â”€ Indexed documents
                       â”œâ”€â”€ Failed attempts
                       â”œâ”€â”€ Quality metrics
                       â””â”€â”€ Goal progress
```

### 2. Agent's Decision-Making Loop

```
1. CHECK MEMORY
   â†“
   "What have I already done?"
   "What failed before?"
   â†“
2. ASSESS PROGRESS
   â†“
   "How close am I to the goal?"
   "What's the current quality?"
   â†“
3. PLAN
   â†“
   "What should I do next?"
   "Which tool should I use?"
   â†“
4. EXECUTE
   â†“
   Use selected tool
   â†“
5. VALIDATE
   â†“
   "Did it work?"
   "What did I learn?"
   â†“
6. ADAPT
   â†“
   Adjust plan based on results
   â†“
7. REPEAT until goal achieved
```

### 3. Example Agent Session

```
ðŸ¤– Agent: "Starting new session. Let me check my memory first."
â†’ Calls: get_memory_summary()
â†’ Result: "50 documents already indexed, 40 URLs scraped"

ðŸ¤– Agent: "Good! I've made progress. Let me assess current state."
â†’ Calls: assess_progress()
â†’ Result: "50% complete, quality score 0.75, need 50 more docs"

ðŸ¤– Agent: "I need to scrape more content. Let me analyze what's available."
â†’ Calls: analyze_website("irs.gov/retirement")
â†’ Result: "107 URLs found, 40 already scraped, 67 new"

ðŸ¤– Agent: "Perfect! Let me scrape 30 new URLs to get closer to goal."
â†’ Calls: scrape_urls(max_pages=30)
â†’ Result: "Downloaded 28, 2 errors"

ðŸ¤– Agent: "Good progress. Let me check quality before processing."
â†’ Calls: check_content_quality(sample_size=5)
â†’ Result: "Quality score 0.85, content is relevant, should process"

ðŸ¤– Agent: "Quality looks good! Processing all new content to markdown."
â†’ Calls: process_content()
â†’ Result: "28 files processed successfully"

ðŸ¤– Agent: "Now indexing to make content searchable."
â†’ Calls: index_to_database()
â†’ Result: "28 documents indexed, 0 errors"

ðŸ¤– Agent: "Let me verify the indexing quality."
â†’ Calls: verify_indexing(query="401k limits")
â†’ Result: "Found 5 results, quality score 0.9"

ðŸ¤– Agent: "Great! Let me check overall progress."
â†’ Calls: assess_progress()
â†’ Result: "78% complete (78/100 docs), quality 0.82, status: in_progress"

ðŸ¤– Agent: "Almost there! Scraping remaining URLs..."
â†’ Continues autonomously...

ðŸ¤– Agent: "Goal achieved! 105 documents indexed, quality 0.85. Task complete."
```

## Agent Tools Reference

### 1. `analyze_website(url: str)`
**Purpose**: Discover available content
**Agent uses when**: Starting a new scraping session
**Returns**: Page count, topics found, quality score, recommendation

### 2. `scrape_urls(max_pages: int)`
**Purpose**: Download HTML content
**Agent uses when**: New content needs to be downloaded
**Returns**: Download statistics (downloaded, skipped, errors)

### 3. `check_content_quality(sample_size: int)`
**Purpose**: Assess if content is worth processing
**Agent uses when**: After downloading, before processing
**Returns**: Quality score, relevance check, processing recommendation

### 4. `process_content()`
**Purpose**: Convert HTML to markdown
**Agent uses when**: Quality check passes
**Returns**: Processing statistics (processed, errors)

### 5. `index_to_database()`
**Purpose**: Make content searchable
**Agent uses when**: Markdown files ready for indexing
**Returns**: Indexing statistics (indexed, errors)

### 6. `verify_indexing(query: str)`
**Purpose**: Test search quality
**Agent uses when**: After indexing, to validate
**Returns**: Search results count, quality score

### 7. `search_knowledge_base(query: str, limit: int)`
**Purpose**: Explore indexed content
**Agent uses when**: Investigating coverage or gaps
**Returns**: Search results with previews

### 8. `assess_progress()`
**Purpose**: Check goal progress
**Agent uses when**: Frequently, to decide next action
**Returns**: Progress %, status, recommendation

### 9. `get_memory_summary()`
**Purpose**: Review past actions
**Agent uses when**: Starting session, avoiding duplicates
**Returns**: Session history, scraped URLs, indexed docs

## Agent Memory System

The agent maintains persistent memory across sessions:

```json
{
  "sessions": [
    {
      "timestamp": "2025-12-30T10:15:00",
      "event_type": "scrape",
      "data": {"downloaded": 30, "errors": 2}
    }
  ],
  "scraped_urls": ["url1", "url2", ...],
  "indexed_documents": [
    {
      "filename": "001_retirement-plans.md",
      "title": "Retirement Plans Overview",
      "url": "https://..."
    }
  ],
  "failed_attempts": [],
  "knowledge_gaps": ["SIMPLE IRA", "SEP IRA"],
  "quality_metrics": {
    "content_quality": 0.85,
    "search_quality": 0.82
  },
  "goal_progress": {
    "documents_indexed": 78,
    "progress_percentage": 78.0,
    "status": "in_progress",
    "recommendation": "Continue scraping"
  }
}
```

## Running the Agent

### Basic Usage
```bash
cd retirement_glossary_scraper
uv run main_agent.py
```

The agent will:
1. âœ… Check its memory for past work
2. âœ… Assess current progress
3. âœ… Plan its approach
4. âœ… Execute autonomously
5. âœ… Adapt based on results
6. âœ… Stop when goal achieved

### Resuming a Session

The agent automatically resumes from where it left off:
```bash
# First run - scrapes 50 docs
uv run main_agent.py

# Interrupt (Ctrl+C)

# Second run - remembers 50 docs, continues from there
uv run main_agent.py
```

### Clearing Memory

To start fresh:
```python
from src.agent_memory import AgentMemory

memory = AgentMemory()
memory.clear_memory()
```

## Agent Configuration

The agent uses the same config as the pipeline:

```python
# src/config.py
class ScraperConfig:
    skip_existing_raw = True      # Agent respects this
    process_content = False        # Agent decides when to process
    index_to_chromadb = True      # Agent decides when to index
    download_delay = 2            # Rate limiting
    # ...
```

## Advantages of Agent vs Pipeline

### Agent Advantages:
1. **Intelligent** - Makes decisions based on context
2. **Efficient** - Avoids duplicate work via memory
3. **Adaptive** - Changes approach if something fails
4. **Goal-oriented** - Knows when it's done
5. **Self-validating** - Checks quality at each step
6. **Resumable** - Can continue from interruption

### Pipeline Advantages:
1. **Predictable** - Always does the same thing
2. **Simple** - Easy to understand
3. **Fast** - No decision overhead
4. **Deterministic** - Same input = same output

## When to Use Each

**Use Autonomous Agent (`main_agent.py`) when:**
- âœ… You want intelligent, adaptive behavior
- âœ… You need to resume interrupted work
- âœ… Content quality varies and needs assessment
- âœ… You want progress tracking and reporting
- âœ… You're building a large knowledge base incrementally

**Use Pipeline (`main.py`) when:**
- âœ… You need predictable, repeatable behavior
- âœ… You're processing a fixed dataset
- âœ… Speed is more important than intelligence
- âœ… You want simple debugging

**Use Legacy (`local_agent_web_scraper.py`) when:**
- âœ… You need exact original behavior
- âœ… Testing compatibility

## Technical Details

**Agent LLM**: Ollama llama3.2:latest
- Reasoning and planning
- Tool selection
- Result interpretation

**Tools**: Agno framework `@tool` decorators
- Type-safe function signatures
- Automatic documentation
- Error handling

**Memory**: JSON-based persistent storage
- Survives restarts
- Human-readable
- Easy to inspect/modify

## Future Enhancements

Potential agent improvements:
- Multi-agent collaboration (scraper + quality checker)
- Learning from user feedback
- Dynamic goal adjustment
- Parallel execution of independent tools
- More sophisticated planning (tree search, MCTS)
- Integration with external knowledge sources
