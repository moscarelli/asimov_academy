# IRS Retirement Topics Glossary - Web Scraper & Knowledge Base

This project scrapes IRS retirement topics, processes them into markdown, and indexes them into a ChromaDB vector database for semantic search.

**New**: The project has been refactored into a modular architecture! See [MODULES.md](MODULES.md) for detailed documentation.

## ğŸ“ Project Structure

```
retirement_glossary_scraper/
â”œâ”€â”€ main_agent.py                   # ğŸ¤– NEW - Autonomous agent entry point
â”œâ”€â”€ main.py                         # Modular pipeline entry point
â”œâ”€â”€ local_agent_web_scraper.py      # Original monolithic script (legacy)
â”œâ”€â”€ query_retirement_glossary.py    # Query tool for searching the knowledge base
â”œâ”€â”€ src/                            # Modular package components
â”‚   â”œâ”€â”€ __init__.py                 # Package initialization
â”‚   â”œâ”€â”€ config.py                   # Configuration management
â”‚   â”œâ”€â”€ scraper.py                  # Web scraping logic
â”‚   â”œâ”€â”€ processor.py                # HTML to markdown processing
â”‚   â”œâ”€â”€ indexer.py                  # ChromaDB indexing
â”‚   â”œâ”€â”€ utils.py                    # Utility functions
â”‚   â”œâ”€â”€ agent_core.py               # ğŸ¤– Autonomous agent with reasoning
â”‚   â”œâ”€â”€ agent_tools.py              # ğŸ¤– Agent's 9 capabilities/tools
â”‚   â””â”€â”€ agent_memory.py             # ğŸ¤– Persistent memory system
â”œâ”€â”€ out/
â”‚   â””â”€â”€ irs_retirement_topics/
â”‚       â”œâ”€â”€ raw/                    # Downloaded HTML files + JSON metadata
â”‚       â”œâ”€â”€ processed/              # AI-processed markdown files
â”‚       â””â”€â”€ discovered_urls.txt     # List of all scraped URLs
â””â”€â”€ tmp/
    â”œâ”€â”€ chroma_retirement_glossary/ # ChromaDB vector database
    â””â”€â”€ agent_memory.json           # ğŸ¤– Agent's persistent memory
```

## ğŸš€ Features

### ğŸ¤– Autonomous Agent Mode (NEW!)

True AI agent that autonomously builds the knowledge base:

- **Plans its own approach** - Decides what steps to take
- **Adapts to results** - Changes strategy based on outcomes
- **Has memory** - Remembers past actions and learns
- **Goal-oriented** - Works until objective is achieved
- **Self-monitoring** - Validates quality and progress
- **Tool-based** - Uses 9 different capabilities strategically

**Agent Components:**
- **agent_core.py**: Autonomous agent with reasoning and planning
- **agent_tools.py**: 9 tools the agent can choose to use
- **agent_memory.py**: Persistent memory across sessions

### Modular Architecture

The scraper is organized into clean, reusable components:

- **config.py**: Centralized configuration with dataclass
- **scraper.py**: Web scraping and HTML downloading
- **processor.py**: AI-powered HTML to markdown conversion
- **indexer.py**: ChromaDB vector database indexing
- **utils.py**: Common utility functions

### Pipeline Modes

**Traditional Pipeline** (`main.py` or `local_agent_web_scraper.py`):

- **Step 1**: Discovers all retirement topic URLs from IRS website
- **Step 2**: Downloads raw HTML content with metadata
- **Step 3**: Optional 20-second countdown before processing
- **Step 4**: Processes HTML to clean markdown using AI agent
- **Step 5**: Indexes markdown files to ChromaDB for semantic search

**Configuration (in `src/config.py`):**
```python
SKIP_EXISTING_RAW = True      # Skip already downloaded files
PROCESS_CONTENT = False        # Enable HTMLâ†’Markdown processing
WAIT_BEFORE_PROCESSING = 20   # Countdown timer (seconds)
INDEX_TO_CHROMADB = True      # Enable ChromaDB indexing
```
INDEX_TO_CHROMADB = True      # Enable ChromaDB indexing
```

### Query Tool (`query_retirement_glossary.py`)

Semantic search interface for the knowledge base:
- Pre-defined queries (401k limits, RMDs, early withdrawals)
- Interactive mode for custom queries
- Displays full metadata and matched content

## ğŸ“Š Current Status
 (Modular Version - Recommended)
```powershell
cd retirement_glossary_scraper
uv run main.py
```

### Run the Scraper (Legacy Monolithic Version)
- **Total URLs**: 107 discovered
- **Downloaded**: 106 HTML files (1 returned 404)
- **Processed**: 106 markdown files
- **Indexed**: 106 documents in ChromaDB

## ğŸ› ï¸ Usage

**Important**: Always run scripts from inside the `retirement_glossary_scraper/` directory.

### Run the Scraper
```powershell
cd retirement_glossary_scraper
uv run local_agent_web_scraper.py
```

### Query the Knowledge Base
```powershell
cd retirement_glossary_scraper
uv run query_retirement_glossary.py
```

Interactive mode allows you to search for any retirement-related topic:
```
Query: What are Roth IRA contribution limits?
Query: Required minimum distribution age
Query: quit  # Exit
```

## ğŸ“‹ Dependencies

- **Agno Framework**: Agent, Ollama, ChromaDB, Knowledge, TextReader
- **BeautifulSoup4**: HTML parsing
- **Requests**: HTTP client
- **Ollama**: Local LLM (llama3.2:latest) for processing and embeddings
  - Host: http://localhost:11434
  - Embedding dimensions: 3072

## ğŸ“ Data Files

### Raw HTML Files (`out/irs_retirement_topics/raw/`)
- Numbered format: `001_retirement-plans.html`
- JSON metadata: download time, source URL, file size, status code

### Processed Markdown (`out/irs_retirement_topics/processed/`)
- YAML frontmatter with metadata
- Clean, structured content
- Document titles extracted

### ChromaDB (`tmp/chroma_retirement_glossary/`)
- Collection: `irs_retirement_glossary`
- SQLite-based persistent storage
- Full-text and semantic search enabled

## ï¿½ Documentation
- **[AGENT.md](AGENT.md)**: ğŸ¤– Complete guide to the autonomous agent
  - How the agent thinks and makes decisions
  - Tool system and capabilities
  - Memory system and persistence
  - Agent vs Pipeline comparison
  - Example sessions and workflows
  - **[MODULES.md](MODULES.md)**: Detailed documentation of the modular architecture
  - Individual module descriptions
  - Usage examples for each component
  - Migration guide from monolithic to modular
  - Best practices and future enhancements

## ï¿½ğŸ” Example Queries

- "401k contribution limits"
- "Required minimum distributions"
- Architecture**: Modular Python package with clear separation of concerns
- **Configuration**: Dataclass-based settings in `config.py`
- **Scraping**: Request-based HTML downloader with rate limiting
- **Processing**: AI agent for content extraction and normalization
- **Indexing**: ChromaDB with Ollama embeddings for semantic search
- **Utilities**: Helper functions for formatting and display

**LLM Model**: Ollama llama3.2:latest
- Content processing: Converts HTML to clean markdown
- Embeddings: 3072-dimensional vectors for semantic search
- Timeout: 60 seconds

**Storage**:
- Raw HTML preserved for reprocessing
- Markdown for readability and ChromaDB indexing
- JSON metadata for tracking and lineage

**Rate Limiting**: 
- 2-second delay between downloads to respect IRS servers
- 0.5-second delay between indexing operation
**Storage**:
- Raw HTML preserved for reprocessing
- Markdown for readability and ChromaDB indexing
- JSON metadata for tracking and lineage

**Rate Limiting**: 2-second delay between downloads to respect IRS servers

## ğŸ“Œ Notes

- Link order preserved from original IRS page (alphabetical)
- Skips non-English language variants
- Handles 404 errors gracefully
- All operations logged with LOG: prefix
- Unicode characters replaced with ASCII for PowerShell compatibility
